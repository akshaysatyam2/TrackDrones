{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Simple CNN Model for classification"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T11:38:53.484714Z","iopub.status.busy":"2024-05-03T11:38:53.483874Z","iopub.status.idle":"2024-05-03T11:52:12.009388Z","shell.execute_reply":"2024-05-03T11:52:12.007879Z","shell.execute_reply.started":"2024-05-03T11:38:53.484679Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 127 images belonging to 2 classes.\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/25\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:54\u001b[0m 38s/step - accuracy: 0.4688 - loss: 0.7345"]},{"name":"stderr","output_type":"stream","text":["WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1714736390.995877     115 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","W0000 00:00:1714736391.012791     115 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.4534 - loss: 8.5504 \n","Epoch 2/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 40ms/step - accuracy: 0.5807 - loss: 0.6728\n","Epoch 3/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 41ms/step - accuracy: 0.5902 - loss: 0.6493\n","Epoch 4/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 41ms/step - accuracy: 0.5928 - loss: 0.6477\n","Epoch 5/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 42ms/step - accuracy: 0.6450 - loss: 0.6095\n","Epoch 6/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 40ms/step - accuracy: 0.7804 - loss: 0.5557\n","Epoch 7/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 41ms/step - accuracy: 0.6983 - loss: 0.5590\n","Epoch 8/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 41ms/step - accuracy: 0.7298 - loss: 0.5149\n","Epoch 9/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 40ms/step - accuracy: 0.7175 - loss: 0.5080\n","Epoch 11/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 41ms/step - accuracy: 0.8365 - loss: 0.4101\n","Epoch 12/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 40ms/step - accuracy: 0.7857 - loss: 0.4018\n","Epoch 13/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 44ms/step - accuracy: 0.7854 - loss: 0.4062\n","Epoch 14/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - accuracy: 0.8188 - loss: 0.4063\n","Epoch 15/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 41ms/step - accuracy: 0.8318 - loss: 0.3653\n","Epoch 16/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 40ms/step - accuracy: 0.8293 - loss: 0.3653\n","Epoch 17/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 40ms/step - accuracy: 0.8547 - loss: 0.3375\n","Epoch 18/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 41ms/step - accuracy: 0.8934 - loss: 0.2992\n","Epoch 19/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 40ms/step - accuracy: 0.8869 - loss: 0.2648\n","Epoch 20/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 42ms/step - accuracy: 0.9279 - loss: 0.2192\n","Epoch 24/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 41ms/step - accuracy: 0.9289 - loss: 0.2142\n","Epoch 25/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 39ms/step - accuracy: 0.9132 - loss: 0.1779\n","{'Drone': 0, 'Not Drone': 1}\n","Found 11 images belonging to 1 classes.\n"]},{"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 48\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(training_set\u001b[38;5;241m.\u001b[39mclass_indices)\n\u001b[1;32m     42\u001b[0m test_generator \u001b[38;5;241m=\u001b[39m train_datagen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/drone-and-not-drone/val\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     44\u001b[0m     target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m),\n\u001b[1;32m     45\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     46\u001b[0m     class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(test_generator)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","train_datagen = ImageDataGenerator(rescale = 1./255,\n","                                   shear_range = 0.2,\n","                                   zoom_range = 0.2,\n","                                   horizontal_flip = True)\n","\n","training_set = train_datagen.flow_from_directory('/kaggle/input/drone-and-not-drone/train',\n","                                                 target_size = (224, 224),\n","                                                 batch_size = 32,\n","                                                 class_mode = 'binary')\n","\n","cnn = tf.keras.models.Sequential()\n","\n","cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[224, 224, 3]))\n","cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n","\n","cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n","cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n","\n","cnn.add(tf.keras.layers.Flatten())\n","\n","cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n","\n","cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n","\n","cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","\n","cnn.fit(training_set, epochs = 25)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T12:14:36.928522Z","iopub.status.busy":"2024-05-03T12:14:36.928155Z","iopub.status.idle":"2024-05-03T12:14:40.948760Z","shell.execute_reply":"2024-05-03T12:14:40.947895Z","shell.execute_reply.started":"2024-05-03T12:14:36.928486Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'Drone': 0, 'Not Drone': 1}\n","Found 11 images belonging to 1 classes.\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.7273 - loss: 1.1845\n","Test accuracy: 0.7272727489471436\n"]}],"source":["import os, numpy as np\n","from tensorflow.keras.preprocessing import image\n","\n","test_folder = \"/kaggle/input/drone-and-not-drone/val/Drone/\"\n","\n","print(training_set.class_indices)\n","\n","test_generator = train_datagen.flow_from_directory(\n","    '/kaggle/input/drone-and-not-drone/val',\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='binary')\n","\n","loss, accuracy = cnn.evaluate(test_generator)\n","print(f'Test accuracy: {accuracy}')"]},{"cell_type":"markdown","metadata":{},"source":["# Fine tuning ResNet50 Model for classification"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T12:15:05.236834Z","iopub.status.busy":"2024-05-03T12:15:05.236429Z","iopub.status.idle":"2024-05-03T12:28:41.905915Z","shell.execute_reply":"2024-05-03T12:28:41.904928Z","shell.execute_reply.started":"2024-05-03T12:15:05.236805Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Found 127 images belonging to 2 classes.\n","Epoch 1/25\n","\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:24\u001b[0m 48s/step - accuracy: 0.4688 - loss: 0.7015"]},{"name":"stderr","output_type":"stream","text":["W0000 00:00:1714738571.348101     113 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.4922 - loss: 0.7399 "]},{"name":"stderr","output_type":"stream","text":["W0000 00:00:1714738580.261883     116 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 3s/step - accuracy: 0.4906 - loss: 0.7408\n","Epoch 2/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 94ms/step - accuracy: 0.5184 - loss: 0.7010\n","Epoch 3/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 96ms/step - accuracy: 0.5676 - loss: 0.6917\n","Epoch 4/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 87ms/step - accuracy: 0.6376 - loss: 0.6724\n","Epoch 5/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 87ms/step - accuracy: 0.6482 - loss: 0.6670\n","Epoch 6/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 92ms/step - accuracy: 0.6083 - loss: 0.6776\n","Epoch 7/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 87ms/step - accuracy: 0.5487 - loss: 0.6702\n","Epoch 8/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.4882 - loss: 0.6760\n","Epoch 9/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 86ms/step - accuracy: 0.5684 - loss: 0.6598\n","Epoch 10/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 97ms/step - accuracy: 0.5236 - loss: 0.6716\n","Epoch 11/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 95ms/step - accuracy: 0.7192 - loss: 0.6459\n","Epoch 12/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 99ms/step - accuracy: 0.5944 - loss: 0.6367\n","Epoch 13/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 94ms/step - accuracy: 0.7087 - loss: 0.6379\n","Epoch 14/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 88ms/step - accuracy: 0.6869 - loss: 0.6131\n","Epoch 15/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 92ms/step - accuracy: 0.6096 - loss: 0.6225\n","Epoch 16/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 91ms/step - accuracy: 0.7707 - loss: 0.6183\n","Epoch 17/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 88ms/step - accuracy: 0.6597 - loss: 0.6224\n","Epoch 18/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 89ms/step - accuracy: 0.6891 - loss: 0.6047\n","Epoch 19/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 91ms/step - accuracy: 0.6901 - loss: 0.6013\n","Epoch 20/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 97ms/step - accuracy: 0.6542 - loss: 0.6068\n","Epoch 21/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 95ms/step - accuracy: 0.6806 - loss: 0.5954\n","Epoch 22/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 92ms/step - accuracy: 0.7177 - loss: 0.5834\n","Epoch 23/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 90ms/step - accuracy: 0.8277 - loss: 0.5847\n","Epoch 24/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 100ms/step - accuracy: 0.6373 - loss: 0.5946\n","Epoch 25/25\n","\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 99ms/step - accuracy: 0.6782 - loss: 0.5940\n","Found 11 images belonging to 1 classes.\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - accuracy: 0.6364 - loss: 0.7229\n","Test accuracy: 0.6363636255264282\n"]},{"name":"stderr","output_type":"stream","text":["W0000 00:00:1714739321.892119     116 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","resnet_base = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n","\n","for layer in resnet_base.layers:\n","    layer.trainable = False\n","\n","x = GlobalAveragePooling2D()(resnet_base.output)\n","x = Dense(128, activation='relu')(x)\n","predictions = Dense(1, activation='sigmoid')(x)\n","\n","model = Model(inputs=resnet_base.input, outputs=predictions)\n","\n","model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n","\n","train_datagen = ImageDataGenerator(rescale=1./255)\n","\n","train_generator = train_datagen.flow_from_directory(\n","    '/kaggle/input/drone-and-not-drone/train',\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='binary')\n","\n","history = model.fit(train_generator, epochs=25)\n","\n","test_generator = train_datagen.flow_from_directory(\n","    '/kaggle/input/drone-and-not-drone/val',\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='binary')\n","\n","loss, accuracy = model.evaluate(test_generator)\n","print(f'Test accuracy: {accuracy}')"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4926150,"sourceId":8292342,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":4}
