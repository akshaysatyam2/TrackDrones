{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8308075,"sourceType":"datasetVersion","datasetId":4934803}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# XML to  tf records conversion","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport tensorflow as tf\nimport xml.etree.ElementTree as et\n\nVOC_LABELS = {\n    'none': (0, 'Background'),\n    'Drone': (1, 'Drone'),\n}\n\nDIRECTORY_ANNOTATIONS = 'labels/'\nDIRECTORY_IMAGES = 'images/'\n\nRANDOM_SEED = 4242\nSAMPLES_PER_FILES = 1\n\n\ndef int64_feature(value):\n    if not isinstance(value, list):\n        value = [value]\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef float_feature(value):\n    if not isinstance(value, list):\n        value = [value]\n    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\n\ndef bytes_feature(value):\n    if not isinstance(value, list):\n        value = [value]\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n\n\ndef _process_image(directory, name):\n    filename = os.path.join(directory, DIRECTORY_IMAGES, name + '.jpg')  # Changed file extension to .jpg\n    image_data = tf.io.gfile.GFile(filename, 'rb').read()\n\n    filename = os.path.join(directory, DIRECTORY_ANNOTATIONS, name + '.xml')\n    tree = et.parse(filename)\n    root = tree.getroot()\n\n    size = root.find('size')\n    shape = [int(size.find('height').text),\n             int(size.find('width').text),\n             int(size.find('depth').text)]\n\n    bboxes = []\n    labels = []\n    labels_text = []\n    difficult = []\n    truncated = []\n\n    for obj in root.findall('object'):\n        label = obj.find('name').text\n        labels.append(int(VOC_LABELS[label][0]))\n        labels_text.append(label.encode('ascii'))\n\n        difficult.append(int(obj.find('difficult').text) if obj.find('difficult') is not None else 0)\n        truncated.append(int(obj.find('truncated').text) if obj.find('truncated') is not None else 0)\n\n        bbox = obj.find('bndbox')\n        bboxes.append((\n            max(float(bbox.find('ymin').text) / shape[0], 0.0),\n            max(float(bbox.find('xmin').text) / shape[1], 0.0),\n            min(float(bbox.find('ymax').text) / shape[0], 1.0),\n            min(float(bbox.find('xmax').text) / shape[1], 1.0)\n        ))\n\n    return image_data, shape, bboxes, labels, labels_text, difficult, truncated\n\n\ndef _convert_to_example(image_data, labels, labels_text, bboxes, shape,\n                        difficult, truncated):\n    xmin = []\n    ymin = []\n    xmax = []\n    ymax = []\n    for b in bboxes:\n        assert len(b) == 4\n        [l.append(point) for l, point in zip([ymin, xmin, ymax, xmax], b)]\n\n    image_format = b'JPEG'\n    example = tf.train.Example(features=tf.train.Features(feature={\n        'image/height': int64_feature(shape[0]),\n        'image/width': int64_feature(shape[1]),\n        'image/channels': int64_feature(shape[2]),\n        'image/shape': int64_feature(shape),\n        'image/object/bbox/xmin': float_feature(xmin),\n        'image/object/bbox/xmax': float_feature(xmax),\n        'image/object/bbox/ymin': float_feature(ymin),\n        'image/object/bbox/ymax': float_feature(ymax),\n        'image/object/bbox/label': int64_feature(labels),\n        'image/object/bbox/label_text': bytes_feature(labels_text),\n        'image/object/bbox/difficult': int64_feature(difficult),\n        'image/object/bbox/truncated': int64_feature(truncated),\n        'image/format': bytes_feature(image_format),\n        'image/encoded': bytes_feature(image_data)}))\n    return example\n\n\ndef _add_to_tfrecord(dataset_dir, name, tfrecord_writer):\n    image_data, shape, bboxes, labels, labels_text, difficult, truncated = \\\n        _process_image(dataset_dir, name)\n    example = _convert_to_example(image_data, labels, labels_text,\n                                  bboxes, shape, difficult, truncated)\n    tfrecord_writer.write(example.SerializeToString())\n\n\ndef _get_output_filename(output_dir, name, idx):\n    return '%s/%s_%03d.tfrecord' % (output_dir, name, idx)\n\n\ndef run(dataset_dir, output_dir, name='voc_train', shuffling=False):\n    if not tf.io.gfile.exists(output_dir):\n        tf.io.gfile.makedirs(output_dir)\n\n    path = os.path.join(dataset_dir, DIRECTORY_ANNOTATIONS)\n    filenames = sorted(os.listdir(path))\n    if shuffling:\n        random.seed(RANDOM_SEED)\n        random.shuffle(filenames)\n\n    i = 0\n    fidx = 0\n    while i < len(filenames):\n        tf_filename = _get_output_filename(output_dir, name, fidx)\n        with tf.io.TFRecordWriter(tf_filename) as tfrecord_writer:\n            j = 0\n            while i < len(filenames) and j < SAMPLES_PER_FILES:\n                sys.stdout.write('converting image%d / %d \\n' % (i + 1, len(filenames)))\n                sys.stdout.flush()\n\n                filename = filenames[i]\n                img_name = filename[:-4]\n                print(img_name)\n                _add_to_tfrecord(dataset_dir, img_name, tfrecord_writer)\n                i += 1\n                j += 1\n            fidx += 1\n\n    print('\\nFinished converting the Pascal VOC dataset!')\n\n\ndataset_dir = \"/kaggle/input/drones-ssd/\"\noutput_dir = \"/kaggle/working/tfrecords/\"\nname = \"voc_2007_train\"\n\n\ndef main():\n    run(dataset_dir, output_dir, name)\n\n\nif __name__ == '__main__':\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T03:42:23.512267Z","iopub.execute_input":"2024-05-04T03:42:23.513193Z","iopub.status.idle":"2024-05-04T03:42:25.051087Z","shell.execute_reply.started":"2024-05-04T03:42:23.513149Z","shell.execute_reply":"2024-05-04T03:42:25.050112Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"converting image1 / 81 \nadedotun-adegborioye-P2CAGKbkZ3U-unsplash\nconverting image2 / 81 \nasael-pena-VX9ZpjSED88-unsplash\nconverting image3 / 81 \nbruno-yamazaky-7HmGgnVBRYg-unsplash\nconverting image4 / 81 \nclay-banks-0VfnZbQd98c-unsplash\nconverting image5 / 81 \ngustaf-von-zeipel-NsrkkaxBIQA-unsplash\nconverting image6 / 81 \nistockphoto-1261285467-1024x1024\nconverting image7 / 81 \nistockphoto-1294781161-1024x1024\nconverting image8 / 81 \nistockphoto-1306713348-1024x1024\nconverting image9 / 81 \nistockphoto-1306713383-1024x1024\nconverting image10 / 81 \nistockphoto-1394026680-1024x1024\nconverting image11 / 81 \nistockphoto-492683865-1024x1024\nconverting image12 / 81 \nistockphoto-514472646-1024x1024\nconverting image13 / 81 \nistockphoto-515197120-1024x1024\nconverting image14 / 81 \nistockphoto-515197120-612x612\nconverting image15 / 81 \nistockphoto-537269404-1024x1024\nconverting image16 / 81 \nistockphoto-541307864-1024x1024\nconverting image17 / 81 \nistockphoto-586731670-1024x1024\nconverting image18 / 81 \nistockphoto-599953668-1024x1024\nconverting image19 / 81 \nistockphoto-638722382-1024x1024\nconverting image20 / 81 \nistockphoto-701045166-1024x1024\nconverting image21 / 81 \nistockphoto-890004548-1024x1024\nconverting image22 / 81 \nistockphoto-935726026-1024x1024\nconverting image23 / 81 \nistockphoto-962832550-1024x1024\nconverting image24 / 81 \njakob-owens-ewzL1sbiLZI-unsplash\nconverting image25 / 81 \nkal-visuals-Ve7iqUNkGsA-unsplash\nconverting image26 / 81 \nmarc-antoine-dube-eVKYAtE01I8-unsplash\nconverting image27 / 81 \nmarkus-spiske-mnsMQjcAcbk-unsplash\nconverting image28 / 81 \nmartin-sanchez-Mz971YWguqE-unsplash\nconverting image29 / 81 \nmatthew-lejune-PyAchlfq21k-unsplash\nconverting image30 / 81 \npexels-brett-sayles-2314657\nconverting image31 / 81 \npexels-caleboquendo-3037937\nconverting image32 / 81 \npexels-chris-f-38966-2087508\nconverting image33 / 81 \npexels-cottonbro-8059115\nconverting image34 / 81 \npexels-cottonbro-8059116\nconverting image35 / 81 \npexels-cottonbro-8059120\nconverting image36 / 81 \npexels-cottonbro-9940769\nconverting image37 / 81 \npexels-cottonbro-9940771\nconverting image38 / 81 \npexels-darrel-und-217939-1087184\nconverting image39 / 81 \npexels-david-bartus-43782-916015\nconverting image40 / 81 \npexels-davidmcbee-388846\nconverting image41 / 81 \npexels-dephshot-11602798\nconverting image42 / 81 \npexels-enrique-aguilar-hernandez-1280189-2443389\nconverting image43 / 81 \npexels-eric-santoyo-232743-739410\nconverting image44 / 81 \npexels-flo-dnd-989753-2100075\nconverting image45 / 81 \npexels-freestockpro-1093232\nconverting image46 / 81 \npexels-guiirossi-1686868\nconverting image47 / 81 \npexels-harrycunningham-3619877\nconverting image48 / 81 \npexels-inmortal-producciones-108299-336232\nconverting image49 / 81 \npexels-invisiblepower-343238\nconverting image50 / 81 \npexels-jeshoots-com-147458-442587\nconverting image51 / 81 \npexels-jess-vide-4603233\nconverting image52 / 81 \npexels-joshsorenson-1034812\nconverting image53 / 81 \npexels-joshsorenson-378268\nconverting image54 / 81 \npexels-kevinbidwell-2445710\nconverting image55 / 81 \npexels-mathieu-deslauriers-196511431-16806581\nconverting image56 / 81 \npexels-matic-holobar-863384-1757697\nconverting image57 / 81 \npexels-mutecevvil-15842322\nconverting image58 / 81 \npexels-mutecevvil-18444696\nconverting image59 / 81 \npexels-newwaveaerial-13643321\nconverting image60 / 81 \npexels-peepsbeirne-17721107\nconverting image61 / 81 \npexels-peterfazekas-997122\nconverting image62 / 81 \npexels-pixelcop-1590242\nconverting image63 / 81 \npexels-pixelcop-1619858\nconverting image64 / 81 \npexels-pixelcop-1809576\nconverting image65 / 81 \npexels-pok-rie-33563-529598\nconverting image66 / 81 \npexels-pok-rie-33563-690360\nconverting image67 / 81 \npexels-pok-rie-33563-724920\nconverting image68 / 81 \npexels-pok-rie-33563-724921\nconverting image69 / 81 \npexels-rickyrecap-1667499\nconverting image70 / 81 \npexels-rquiros-1895900\nconverting image71 / 81 \npexels-szaboviktor-5378479\nconverting image72 / 81 \npexels-thelazyartist-1170064\nconverting image73 / 81 \npexels-thelazyartist-1170344\nconverting image74 / 81 \npexels-thelazyartist-1488083\nconverting image75 / 81 \npexels-thelazyartist-2527839\nconverting image76 / 81 \npexels-tito-noverian-putra-776625-1838248\nconverting image77 / 81 \npexels-winnie-the-pooh-1010466-2120658\nconverting image78 / 81 \npexels-wkandark-19806135\nconverting image79 / 81 \nsaffu-y7KcgbpAoS8-unsplash\nconverting image80 / 81 \nsam-mcghee-UC5FpqofFOk-unsplash\nconverting image81 / 81 \nval-vesa-21lre-F2yWI-unsplash\n\nFinished converting the Pascal VOC dataset!\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"Provides data for the Pascal VOC Dataset (images + annotations).\"\"\"\n\nimport tensorflow as tf\nfrom datasets import pascalvoc_common\n\nFILE_PATTERN = 'voc_2007_%s_*.tfrecord'\nITEMS_TO_DESCRIPTIONS = {\n    'image': 'A color image of varying height and width.',\n    'shape': 'Shape of the image',\n    'object/bbox': 'A list of bounding boxes, one per each object.',\n    'object/label': 'A list of labels, one per each object.',\n}\n\n# (Images, Objects) statistics on every class.\nTRAIN_STATISTICS = {\n    'none': (0, 0),\n    'headphone': (70, 73),\n}\n\nTEST_STATISTICS = {\n    'none': (0, 0),\n    'Drone': (11, 11),\n}\n\nSPLITS_TO_SIZES = {\n    'train': 70,  # Training data volume\n    'test': 11,    # Test data volume\n}\n\nSPLITS_TO_STATISTICS = {\n    'train': TRAIN_STATISTICS,\n    'test': TEST_STATISTICS,\n}\n\nNUM_CLASSES = 1  # Modify according to the actual category of your own data (without background)\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n    \"\"\"Gets a dataset tuple with instructions for reading ImageNet.\n\n    Args:\n      split_name: A train/test split name.\n      dataset_dir: The base directory of the dataset sources.\n      file_pattern: The file pattern to use when matching the dataset sources.\n        It is assumed that the pattern contains a '%s' string so that the split\n        name can be inserted.\n      reader: The TensorFlow reader type.\n\n    Returns:\n      A `Dataset` namedtuple.\n\n    Raises:\n        ValueError: if `split_name` is not a valid train/test split.\n    \"\"\"\n    if not file_pattern:\n        file_pattern = FILE_PATTERN\n    return pascalvoc_common.get_split(split_name, dataset_dir,\n                                      file_pattern, reader,\n                                      SPLITS_TO_SIZES,\n                                      ITEMS_TO_DESCRIPTIONS,\n                                      NUM_CLASSES)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T03:52:55.099021Z","iopub.execute_input":"2024-05-04T03:52:55.099843Z","iopub.status.idle":"2024-05-04T03:52:55.904067Z","shell.execute_reply.started":"2024-05-04T03:52:55.099808Z","shell.execute_reply":"2024-05-04T03:52:55.902776Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"I0504 03:52:55.447418 137865770477376 config.py:58] PyTorch version 2.1.2 available.\nI0504 03:52:55.449865 137865770477376 config.py:95] TensorFlow version 2.15.0 available.\nI0504 03:52:55.453251 137865770477376 config.py:108] JAX version 0.4.23 available.\nI0504 03:52:55.456191 137865770477376 config.py:122] Apache Beam version 2.46.0 available.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Provides data for the Pascal VOC Dataset (images + annotations).\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pascalvoc_common\n\u001b[1;32m      6\u001b[0m FILE_PATTERN \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoc_2007_\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_*.tfrecord\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m ITEMS_TO_DESCRIPTIONS \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA color image of varying height and width.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape of the image\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject/bbox\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA list of bounding boxes, one per each object.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject/label\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA list of labels, one per each object.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m }\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'pascalvoc_common' from 'datasets' (/opt/conda/lib/python3.10/site-packages/datasets/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'pascalvoc_common' from 'datasets' (/opt/conda/lib/python3.10/site-packages/datasets/__init__.py)","output_type":"error"}]},{"cell_type":"code","source":"\"\"\"Provides data for the Pascal VOC Dataset (images + annotations).\"\"\"\n\nimport os\nimport tensorflow as tf\nfrom datasets import dataset_utils\nimport tensorflow_datasets as tfds\n\nslim = tf.contrib.slim\n\nVOC_LABELS = {\n    'none': (0, 'Background'),\n    'headphone': (1, 'Drone'),\n}\n\ndef get_split(split_name, dataset_dir, file_pattern, reader,\n              split_to_sizes, items_to_descriptions, num_classes):\n    \"\"\"Gets a dataset tuple with instructions for reading Pascal VOC dataset.\n\n    Args:\n      split_name: A train/test split name.\n      dataset_dir: The base directory of the dataset sources.\n      file_pattern: The file pattern to use when matching the dataset sources.\n        It is assumed that the pattern contains a '%s' string so that the split\n        name can be inserted.\n      reader: The TensorFlow reader type.\n\n    Returns:\n      A `Dataset` namedtuple.\n\n    Raises:\n        ValueError: if `split_name` is not a valid train/test split.\n    \"\"\"\n    if split_name not in split_to_sizes:\n        raise ValueError('split name %s was not recognized.' % split_name)\n    file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n    # Allowing None in the signature so that dataset_factory can use the default.\n    if reader is None:\n        reader = tf.data.TFRecordDataset\n    # Features in Pascal VOC TFRecords.\n    keys_to_features = {\n        'image/encoded': tf.io.FixedLenFeature((), tf.string, default_value=''),\n        'image/format': tf.io.FixedLenFeature((), tf.string, default_value='jpeg'),\n        'image/height': tf.io.FixedLenFeature([1], tf.int64),\n        'image/width': tf.io.FixedLenFeature([1], tf.int64),\n        'image/channels': tf.io.FixedLenFeature([1], tf.int64),\n        'image/shape': tf.io.FixedLenFeature([3], tf.int64),\n        'image/object/bbox/xmin': tf.io.VarLenFeature(dtype=tf.float32),\n        'image/object/bbox/ymin': tf.io.VarLenFeature(dtype=tf.float32),\n        'image/object/bbox/xmax': tf.io.VarLenFeature(dtype=tf.float32),\n        'image/object/bbox/ymax': tf.io.VarLenFeature(dtype=tf.float32),\n        'image/object/bbox/label': tf.io.VarLenFeature(dtype=tf.int64),\n        'image/object/bbox/difficult': tf.io.VarLenFeature(dtype=tf.int64),\n        'image/object/bbox/truncated': tf.io.VarLenFeature(dtype=tf.int64),\n    }\n    items_to_handlers = {\n        'image': slim.tfexample_decoder.Image('image/encoded', 'image/format'),\n        'shape': slim.tfexample_decoder.Tensor('image/shape'),\n        'object/bbox': slim.tfexample_decoder.BoundingBox(\n                ['ymin', 'xmin', 'ymax', 'xmax'], 'image/object/bbox/'),\n        'object/label': slim.tfexample_decoder.Tensor('image/object/bbox/label'),\n        'object/difficult': slim.tfexample_decoder.Tensor('image/object/bbox/difficult'),\n        'object/truncated': slim.tfexample_decoder.Tensor('image/object/bbox/truncated'),\n    }\n    decoder = slim.tfexample_decoder.TFExampleDecoder(\n        keys_to_features, items_to_handlers)\n\n    labels_to_names = None\n    if dataset_utils.has_labels(dataset_dir):\n        labels_to_names = dataset_utils.read_label_file(dataset_dir)\n    # else:\n    #     labels_to_names = create_readable_names_for_imagenet_labels()\n    #     dataset_utils.write_label_file(labels_to_names, dataset_dir)\n\n    return tfds.load(\n        name=tfds.Split(split_name),\n        data_dir=file_pattern,\n        with_info=True,\n        as_supervised=True\n    )\n\n","metadata":{},"execution_count":null,"outputs":[]}]}